{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some javascript to enable auto numbering of mathematical equations. [Reference](https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tree/master/src/jupyter_contrib_nbextensions/nbextensions/equation-numbering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n",
       "\n",
       "MathJax.Hub.Queue(\n",
       "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "  [\"PreProcess\", MathJax.Hub],\n",
       "  [\"Reprocess\", MathJax.Hub]\n",
       ");"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});\n",
    "\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will be figuring out the math for a binary logistic classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is similar to Linear Regression but instead of a real valued output $y$, it will be either 0 or 1 since we need to classify into one of 2 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear regression post, we have defined our hypothesis function as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can also have multiple input features i.e $x_1, x_2, x_3...$ and so on, so in that case our hypothesis function becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_1x_3 ....\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added $x_0=1$ with $\\theta_0$ for simplification. Now, the hypothesis function can be expressed as a combination of just 2 vectors: $X=[x_0, x_1, x_2, x_3, ...]$ and $\\theta = [\\theta_0, \\theta_1, \\theta_2, ...]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "h_\\theta(x) = \\theta^TX\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, the output of this function will be a real value, so we'll apply an activation function to convert the output to 0 or 1. We'll use the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) $g(z)$ for this purpose. **TODO: Explore other activation functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h(X) = g(\\theta^TX) = \\frac{1}{1+e^{-\\theta^TX}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used loss function for logistic regression is log-loss (or cross-entropy) **TODO: Why log-loss? Explore other loss functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the loss function $l(\\theta)$ for $m$ training examples is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "l(\\theta) = -\\frac{1}{m}(\\sum_{i=1}^m y^{(i)}log(h(x^{(i)}) + (1-y^{(i)})log(1-h(x^{(i)}))\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can also be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "l(\\theta) = -(\\sum_{i=1}^m y^{(i)}log(g(\\theta^T x^{(i)})) + (1-y^{(i)})log(1-g(\\theta^T x^{(i)}))\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to linear regression, we need to find out the value of $\\theta$ that minimizes the loss. We can again use gradient descent for that. **TODO: Explore other methods to minimize the loss function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} l(\\theta)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From (8), we get that we need to find out $\\frac{\\partial}{\\partial \\theta_j} l(\\theta)$ to derive the gradient descent rule. Lets start by working with just one training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial}{\\partial \\theta_j} l(\\theta)$ can be broken down as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta} l(\\theta) = \\frac{\\partial}{\\partial h(x)}l(\\theta).\\frac{\\partial}{\\partial \\theta}h(x)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta} l(\\theta) = \\frac{\\partial}{\\partial g(\\theta^T x)}l(\\theta).\\frac{\\partial}{\\partial \\theta}g(\\theta^Tx)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating $\\frac{\\partial}{\\partial \\theta}g(\\theta^Tx)$ first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta}g(\\theta^Tx)  = \\frac{\\partial}{\\partial \\theta} \\left(\\frac{1}{1+e^{-\\theta^T x}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\frac{\\partial}{\\partial \\theta}({1+e^{-\\theta^T x}})^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule of derivatives,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=-({1+e^{-\\theta^T x}})^{-2}.(e^{-\\theta^T x}).(-x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\frac{e^{-\\theta^T x}}{(1+e^{-\\theta^T x})^2}.(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\frac{1+e^{-\\theta^T x}-1}{(1+e^{-\\theta^T x})^2}.(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\left(\\frac{1+e^{-\\theta^T x}}{(1+e^{-\\theta^T x})^2}-\\frac{1}{(1+e^{-\\theta^T x})^2}\\right).(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\left(\\frac{1}{(1+e^{-\\theta^T x})}-\\frac{1}{(1+e^{-\\theta^T x})^2}\\right).(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=(g(\\theta^T x)-g(\\theta^T x)^2).(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta}g(\\theta^Tx) =g(\\theta^T x)(1-g(\\theta^T x).x\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculating $\\frac{\\partial}{\\partial g(\\theta^T x)}l(\\theta)$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial g(\\theta^T x)}l(\\theta) = \\frac{\\partial}{\\partial g(\\theta^T x)}.(-(y.log(g(\\theta^T x) + (1-y)log(1-g(\\theta^T x)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, using the chain rule,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= -\\left(\\frac{y}{g(\\theta^T x)} + \\frac{1-y}{1-g(\\theta^T x)}.(-1)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= -\\left(\\frac{y-y.g(\\theta^T x)-g(\\theta^T x)+y.g(\\theta^T x)}{g(\\theta^T x).(1-g(\\theta^T x)}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= -\\left(\\frac{y-g(\\theta^T x)}{g(\\theta^T x).(1-g(\\theta^T x)}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial g(\\theta^T x)}l(\\theta) = -\\left(\\frac{y-g(\\theta^T x)}{g(\\theta^T x).(1-g(\\theta^T x)}\\right)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, combining (10),(11),(12), we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta} l(\\theta) = -\\left(\\frac{y-g(\\theta^T x)}{g(\\theta^T x).(1-g(\\theta^T x)}\\right).g(\\theta^T x)(1-g(\\theta^T x).x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta} l(\\theta) = -(y-g(\\theta^T x)).x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta} l(\\theta) = -(y-h(x)).x\n",
    "\\end{equation}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging this back in (8),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "\\theta_j = \\theta_j + \\alpha(y-h(x)).x\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc-classification-project",
   "language": "python",
   "name": "doc-classification-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
